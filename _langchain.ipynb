{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:15.744062539Z",
     "start_time": "2023-08-10T07:01:15.698528334Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "openai_api_key='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'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:15.744623200Z",
     "start_time": "2023-08-10T07:01:15.701833288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:15.745101305Z",
     "start_time": "2023-08-10T07:01:15.706510127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:15.766800246Z",
     "start_time": "2023-08-10T07:01:15.710561294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:15.797266478Z",
     "start_time": "2023-08-10T07:01:15.757248300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\nBright Feet Socks.'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:16.616516927Z",
     "start_time": "2023-08-10T07:01:15.757450603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "'SockSplash'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:18.526307169Z",
     "start_time": "2023-08-10T07:01:16.618244787Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt templates\n",
    "usually LLMs will add the user input to a larger peice of text, called a prompt template. This provides additional context on the specific task at hand."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "'What is a good name for a company that makes colorful socks?'"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:18.569312108Z",
     "start_time": "2023-08-10T07:01:18.528535416Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - eg you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the [section on prompts](https://python.langchain.com/docs/modules/model_io/prompts) for more detail.\n",
    "\n",
    "PromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc) Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let's take a look at this below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:18.613322570Z",
     "start_time": "2023-08-10T07:01:18.569546402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output Parsers\n",
    "OutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main type of OutputParsers, including:\n",
    "* Convert text from LLM -> structured information (eg JSON)\n",
    "* Convert a ChatMessage into just a string\n",
    "* Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
    "\n",
    "For full information on this, see the section on output parsers\n",
    "\n",
    "In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.OutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main type of OutputParsers, including:\n",
    "\n",
    "* Convert text from LLM -> structured information (eg JSON)\n",
    "* Convert a ChatMessage into just a string\n",
    "* Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
    "\n",
    "For full information on this, see the section on output parsers\n",
    "\n",
    "In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "['hi', 'bye']"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "# >> ['hi', 'bye']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:18.613750170Z",
     "start_time": "2023-08-10T07:01:18.613188086Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLMChain\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "['red', 'blue', 'green', 'yellow', 'purple']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(openai_api_key=openai_api_key),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n",
    "chain.run(\"colors\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T07:01:20.074989467Z",
     "start_time": "2023-08-10T07:01:18.613547155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
